{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80e4a5d",
   "metadata": {},
   "source": [
    "# Transformers Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374c598",
   "metadata": {},
   "source": [
    "References:\n",
    "- [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [NLP with Transformers](https://www.amazon.com.au/Natural-Language-Processing-Transformers-Applications/dp/1098103246)\n",
    "- [Transformer Anatomy](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n",
    "- [Advanced Deep Learning with Python](https://www.amazon.com.au/Advanced-Deep-Learning-Python-Vasilev/dp/178995617X)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/paulaceccon/deep-learning-studies/blob/main/notebooks/generative_models/transformers/transformer_architecture.ipynb\" target=\"_parent\" style=\"float: left;\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423ac196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from math import sqrt\n",
    "from torch import nn\n",
    "from loguru import logger\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3793874",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90c1ca",
   "metadata": {},
   "source": [
    "<img src=\"images/architecture.png\" alt=\"Architecture\" width=\"400\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404a7c1",
   "metadata": {},
   "source": [
    "The encoder in a Transformer model is responsible for processing the input sequence, such as a sentence or a document. It consists of a stack (`Nx`) of encoder layers or \"blocks\". Each encoder layer receives a sequence of token embeddings, which are representations of the input tokens obtained through tokenization and embedding techniques.\n",
    "\n",
    "To capture the sequential nature of the text, the encoder combines the token embeddings with positional embeddings. Positional embeddings provide information about the relative positions of the tokens in the input sequence. This injection of positional information helps the attention mechanism in the Transformer model to understand the order of the tokens.\n",
    "\n",
    "Each encoder layer in the stack performs the following operations on the input embeddings:\n",
    "\n",
    "1. **Multi-head self-attention layer:** This layer allows each token to attend to other tokens in the input sequence. It computes attention weights that determine the importance of each token with respect to other tokens. The self-attention mechanism helps the model capture dependencies and relationships between different tokens in the input sequence. Because the self-attention mechanism works across the whole input sequence, the encoder is **bidirectional** by design.\n",
    "\n",
    "2. **Fully connected feed-forward layer:** After the self-attention layer, the output embeddings from the previous step are passed through a feed-forward neural network layer. This layer applies a non-linear transformation to each input embedding independently. The feed-forward layer introduces additional modeling capacity and helps capture more complex relationships within the input sequence. This layer can be defined by:\n",
    "\n",
    "$$FFN(x) = ReLU(\\mathbf{W}_1 x +b_1)\\mathbf{W}_2 + b_2$$\n",
    "\n",
    "In addition to the self-attention and feed-forward layers, each encoder layer in the stack incorporates a layer normalization step. Layer normalization is applied after each sublayer, such as the self-attention layer and the feed-forward layer. The purpose of layer normalization is to normalize the values across the features dimension (often referred to as the hidden or inner dimension) for each position in the sequence.\n",
    "\n",
    "By applying layer normalization, the model ensures that the mean of the values across the features dimension is close to zero, and the standard deviation is close to one. This normalization step helps address issues related to internal covariate shift, which is the phenomenon of the distribution of inputs to a learning system changing during training, causing difficulties in learning.\n",
    "\n",
    "The output embeddings of each encoder layer have the same size as the inputs. The role of the encoder stack is to \"update\" the input embeddings at each layer, gradually incorporating contextual information and capturing higher-level representations of the sequence. The stacking of multiple encoder layers allows the model to learn hierarchical representations of the input, capturing both local and global dependencies.\n",
    "\n",
    "The encoder's final output is then passed to the decoder for further processing. In a sequence-to-sequence setting, the decoder generates predictions for the next token in the sequence based on the encoder's output. This prediction is then fed back into the decoder to generate subsequent tokens, continuing until an end-of-sequence (EOS) token is reached.\n",
    "\n",
    "---\n",
    "\n",
    "The decoder in the Transformer model is structurally similar to the encoder, but it has some key differences. The input at each step of the decoder is its own predicted output word from the previous step, similar to an autoregressive model. The input word is embedded and combined with positional encodings, just like in the encoder.\n",
    "\n",
    "The decoder consists of a stack of N identical blocks. Each block contains three sublayers, and within each sublayer, residual connections and normalization are employed. The three sublayers in each block of the decoder are as follows:\n",
    "\n",
    "1. **Masked multihead self-attention layer**: This self-attention mechanism in the decoder allows each position in the sequence to attend to preceding positions in the partially generated target sequence. Unlike the encoder's self-attention, which attends to the entire input sequence, the decoder's self-attention _only attends to the preceding sequence elements_. This is achieved by applying a _mask_ to the softmax input, setting the corresponding values to -âˆž, which prevents illegal connections between future positions and the current position being attended to. This masking ensures that the decoder is unidirectional, attending only to the preceding positions.\n",
    "\n",
    "2. **Encoder-decoder attention layer**: This layer allows every position in the decoder to attend over all positions in the input sequence (encoder output). The queries for this attention mechanism come from the previous decoder layer, while the keys and values are derived from the previous sublayer output, which represents the processed decoder output from the previous step. This attention mechanism mimics the typical encoder-decoder attention mechanisms used in sequence-to-sequence models with attention.\n",
    "\n",
    "3. **Feed-forward network**: Similar to the encoder, the decoder includes a feed-forward network. This network applies a non-linear transformation to each position's representation independently, enhancing the model's ability to capture complex relationships within the decoder.\n",
    "\n",
    "The decoder concludes with a fully connected layer, followed by a softmax activation function. This final layer predicts the most probable next word of the sentence based on the representations and attention mechanisms of the decoder.\n",
    "\n",
    "To regularize the model and prevent overfitting, dropout is applied in the Transformer model. Dropout is added to the output of each sublayer before it is combined with the sublayer input and normalized. Additionally, dropout is applied to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67500234",
   "metadata": {},
   "source": [
    "## Main Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8bb110",
   "metadata": {},
   "source": [
    "### 1. Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a063a7",
   "metadata": {},
   "source": [
    "#### Scale Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b36019",
   "metadata": {},
   "source": [
    "The scaled dot-product attention is the most common way to implement a self-attention layer. It computes the attention weights between a query vector and a set of key-value pairs by calculating the dot product similarity between them. The key idea behind the scaled dot-product attention is to scale the dot products by the square root of the dimensionality of the query and key vectors, which helps stabilize the gradients during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292134e",
   "metadata": {},
   "source": [
    "<img src=\"images/scale_dot_product.png\" alt=\"Scale Dot Product\" width=\"200\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd3fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor, \n",
    "    key: torch.Tensor, \n",
    "    value: torch.Tensor, \n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    dropout: Optional[nn.Dropout] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute scaled dot product attention weights.\n",
    "\n",
    "    Args:\n",
    "        query: Tensor with shape [batch_size, seq_length_q, depth_q].\n",
    "        key: Tensor with shape [batch_size, seq_length_k, depth_k].\n",
    "        value: Tensor with shape [batch_size, seq_length_v, depth_v].\n",
    "        mask: Optional tensor with shape [batch_size, seq_length_q, seq_length_k],\n",
    "            containing values to be masked. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        Tensor with shape [batch_size, seq_length_q, depth_v].\n",
    "    \"\"\"\n",
    "    dim_k = query.size(-1)\n",
    "    logger.debug(f\"query_size: {query.size()}\")\n",
    "    logger.debug(f\"key: {key.transpose(-2, -1).size()}\")\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        weights = dropout(weights)\n",
    "        \n",
    "    return torch.matmul(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b36943",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936d525",
   "metadata": {},
   "source": [
    "The multi-head attention is an extension of the self-attention mechanism. It enhances the modeling capability by performing multiple attention computations in parallel, with different learned linear projections.\n",
    "\n",
    "The reasoning for heaving multi-head attention is that the softmax of one head usually focuses on mostly a single aspect of similarity. In other words, the multi-head attention allows the model to capture different types of dependencies and relationships between the elements in the input sequence. I.e., each head can attend to different parts of the sequence, enabling the model to learn more nuanced patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f724eaf",
   "metadata": {},
   "source": [
    "<img src=\"images/multi_head_attention.png\" alt=\"Multi-Head Attention\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3079638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the multi-head attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        logger.debug(f\"hidden_dim: {self.embed_dim}\")\n",
    "        logger.debug(f\"num_heads: {self.num_heads}\")\n",
    "        \n",
    "        assert self.embed_dim % self.num_heads == 0\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        logger.debug(f\"head_dim: {self.head_dim}\")\n",
    "        \n",
    "        self.q = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
    "        self.k = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
    "        self.v = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
    "        self.output_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor of shape [batch_size, seq_len, embed_dim].\n",
    "            key: Key tensor of shape [batch_size, seq_len, embed_dim].\n",
    "            value: Value tensor of shape [batch_size, seq_len, embed_dim].\n",
    "            mask: Optional mask tensor. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, embed_dim], \n",
    "            representing the output of the multi-head attention.\n",
    "        \"\"\"\n",
    "        q = self.q(query)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "        logger.debug(f\"q_size: {q.size()}\")\n",
    "        logger.debug(f\"k_size: {k.size()}\")\n",
    "        logger.debug(f\"v_size: {v.size()}\")\n",
    "                     \n",
    "        q = q.view(q.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(k.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(v.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        logger.debug(f\"qT_size: {q.size()}\")\n",
    "        logger.debug(f\"kT_size: {k.size()}\")\n",
    "        logger.debug(f\"vT_size: {v.size()}\")\n",
    "\n",
    "        attn_scores = scaled_dot_product_attention(q, k, v, mask, self.dropout)\n",
    "        attn_scores = attn_scores.transpose(1, 2).contiguous()\n",
    "        attn_scores = attn_scores.view(attn_scores.size(0), -1, self.embed_dim)\n",
    "        logger.debug(f\"attn_scores: {attn_scores.size()}\")\n",
    "\n",
    "        output = self.output_linear(attn_scores)\n",
    "        logger.debug(f\"output_size: {output.size()}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a30bdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-07-27 15:35:44.212\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m12\u001B[0m - \u001B[34m\u001B[1mhidden_dim: 768\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.213\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m13\u001B[0m - \u001B[34m\u001B[1mnum_heads: 12\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.213\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m17\u001B[0m - \u001B[34m\u001B[1mhead_dim: 64\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.221\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m48\u001B[0m - \u001B[34m\u001B[1mq_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.221\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m49\u001B[0m - \u001B[34m\u001B[1mk_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.222\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m50\u001B[0m - \u001B[34m\u001B[1mv_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.222\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m56\u001B[0m - \u001B[34m\u001B[1mqT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.222\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m57\u001B[0m - \u001B[34m\u001B[1mkT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.223\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m58\u001B[0m - \u001B[34m\u001B[1mvT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.223\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m22\u001B[0m - \u001B[34m\u001B[1mquery_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.223\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m23\u001B[0m - \u001B[34m\u001B[1mkey: torch.Size([1, 12, 64, 9])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.224\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mattn_scores: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:35:44.224\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1moutput_size: torch.Size([1, 9, 768])\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "\n",
    "query = key = value = inputs_embeds\n",
    "\n",
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(query, key, value)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954e313",
   "metadata": {},
   "source": [
    "Here's a breakdown of what the code does:\n",
    "\n",
    "1. It applies linear transformations to the query, key, and value tensors using the learned linear layers `self.q`, `self.k`, `and self.v`, respectively. This projects the tensors to the appropriate dimensions for the attention computation.\n",
    "\n",
    "2. The tensors are reshaped and transposed to prepare them for the matrix multiplication step. The view and transpose operations create multi-head versions of the tensors.\n",
    "\n",
    "3. The attention scores are computed by performing matrix multiplication between the query and key tensors. The resulting scores represent the similarity or importance of each element in the query with respect to the elements in the key.\n",
    "\n",
    "4. The attention scores are scaled by dividing them by the square root of the head dimension (`self.head_dim`). This scaling helps stabilize the gradients during training.\n",
    "\n",
    "5. If a mask is provided (`self.mask` is not `None`), the attention scores are masked by setting the scores corresponding to masked positions to negative infinity. This ensures that these masked positions do not contribute to the attention computation.\n",
    "\n",
    "6. The attention scores are passed through a softmax activation function along the last dimension (`dim=-1`). This calculates the attention weights or probabilities for each element in the query with respect to the elements in the key.\n",
    "\n",
    "7. The attention probabilities are used to weight the value tensor. This is done by performing matrix multiplication between the attention probabilities and the value tensor. This step computes the context or output representation based on the attention weights.\n",
    "\n",
    "8. The resulting attention output is transposed and reshaped to match the original shape. This is achieved using the transpose and view operations.\n",
    "\n",
    "9. Finally, the attention output is passed through the `self.output_linear` linear layer, which applies another linear transformation to the output representation.\n",
    "\n",
    "10. The resulting output is returned, representing the output of the multi-head attention operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad8891",
   "metadata": {},
   "source": [
    "### 2. The Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b7fce",
   "metadata": {},
   "source": [
    "The feed-forward layer is a type of neural network layer that processes the input data independently at each position in the input sequence, without considering the dependencies between different positions. This means that the computations for different positions can be parallelized, making the Transformer architecture highly efficient for sequence processing tasks.\n",
    "\n",
    "The feed-forward layer in Transformers typically consists of two linear transformations with a non-linear activation function in between. The input to the feed-forward layer is a tensor representing the hidden states of the previous layer or the input embeddings. \n",
    "\n",
    "The feed-forward layer is a critical component of Transformers as it helps capture local patterns and dependencies in the input data. By incorporating non-linear transformations, it enables the model to learn complex representations and extract meaningful features from the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7b079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward layer module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the feed-forward layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the feed-forward layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim], \n",
    "            representing the output of the feed-forward layer.\n",
    "        \"\"\"\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        logger.debug(f\"ff_output_size: {x.size()}\")\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7d20c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-07-27 15:35:52.429\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m30\u001B[0m - \u001B[34m\u001B[1mff_output_size: torch.Size([1, 9, 768])\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f5608",
   "metadata": {},
   "source": [
    "##  The Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0ce59",
   "metadata": {},
   "source": [
    "Converts an input sequence of tokens into a sequence of embedding vectors (hidden state).\n",
    "It's composed of multiple components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582ca07",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Applied to normalized each input in the batch to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4549fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder block module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the encoder block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer encoder block.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "            mask: Optional mask tensor. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim], \n",
    "            representing the output of the encoder block.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"encoder_block_input_size: {x.size()}\")\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        attention_output = self.attention(hidden_state, hidden_state, hidden_state, mask)\n",
    "        x = x + self.dropout(x)\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        logger.debug(f\"encoder_block_output_size: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0941480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-07-27 15:41:56.168\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m12\u001B[0m - \u001B[34m\u001B[1mhidden_dim: 768\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.172\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m13\u001B[0m - \u001B[34m\u001B[1mnum_heads: 12\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.173\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m17\u001B[0m - \u001B[34m\u001B[1mhead_dim: 64\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.206\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m29\u001B[0m - \u001B[34m\u001B[1mencoder_block_input_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.208\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m48\u001B[0m - \u001B[34m\u001B[1mq_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.208\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m49\u001B[0m - \u001B[34m\u001B[1mk_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.209\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m50\u001B[0m - \u001B[34m\u001B[1mv_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.209\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m56\u001B[0m - \u001B[34m\u001B[1mqT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.209\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m57\u001B[0m - \u001B[34m\u001B[1mkT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.209\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m58\u001B[0m - \u001B[34m\u001B[1mvT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.210\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m22\u001B[0m - \u001B[34m\u001B[1mquery_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.210\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m23\u001B[0m - \u001B[34m\u001B[1mkey: torch.Size([1, 12, 64, 9])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.211\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mattn_scores: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.212\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1moutput_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.217\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m30\u001B[0m - \u001B[34m\u001B[1mff_output_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:41:56.217\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m36\u001B[0m - \u001B[34m\u001B[1mencoder_block_output_size: torch.Size([1, 9, 768])\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = TransformerEncoderBlock(config)\n",
    "_ = encoder_layer(inputs_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ce9fd",
   "metadata": {},
   "source": [
    "1. **Layer Normalization:** The input tensor `x` is first passed through a layer normalization operation using `self.layer_norm_1`. This operation normalizes the activations across the hidden dimension of `x` to have zero mean and unit variance. The result is stored in `hidden_state`.\n",
    "\n",
    "2. **Attention with Skip Connection:** The attention mechanism is applied to `hidden_state` using `self.attention`. This attention operation takes `hidden_state` as the input and produces an attention-based output. The output is then element-wise added (`+`) to the original input tensor `x`. This skip connection allows the model to directly incorporate the original input along with the attention-based output.\n",
    "\n",
    "3. **Feed-Forward Layer with Skip Connection:** The output of the previous step is passed through another layer normalization operation `self.layer_norm_2` to normalize the activations. Then, the result is passed through the feed-forward layer `self.feed_forward`. The output of the feed-forward layer is again element-wise added (`+`) to the input tensor from the previous step (`x`). This skip connection allows the model to combine the information from the original input with the transformed output from the feed-forward layer.\n",
    "\n",
    "In summary, the skip connections enable the model to incorporate the original input tensor `x` into the output of each layer. By adding the transformed outputs to the original input, the model can retain important information from the input and facilitate the flow of gradients during training. The skip connections help in addressing the vanishing gradient problem and make it easier to train deep Transformer architectures by ensuring the model has access to the original input information at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8800b",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ced738",
   "metadata": {},
   "source": [
    "The purpose of positional embeddings is to provide the model with a representation that encodes the relative positions of tokens within the sequence. This allows the model to differentiate between tokens based on their position, even though all tokens initially have the same embeddings.\n",
    "\n",
    "In the original Transformer model, the positional embeddings used to encode the sequential order of tokens are learned as part of the model training process. The positional embeddings are initialized with fixed sinusoidal functions of different frequencies and then fine-tuned during training. The specific form of the positional embeddings in the original Transformer model is as follows:\n",
    "\n",
    "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "\n",
    "\n",
    "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "\n",
    "Here, $P$ represents the matrix of positional embeddings, and $d_{model}$ is the dimensionality of both the token and positional embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4072f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeddings layer module.\n",
    "    Combines a token embedding layer that projects the `input_ids` to a dense hidden state \n",
    "    with the positional embedding that does the same for `position_ids`. \n",
    "    The resulting embedding is simply the sum of both embeddings.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the embeddings layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the embeddings layer.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input tensor of shape [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim], \n",
    "            representing the embeddings of the input.\n",
    "\n",
    "        Notes:\n",
    "            1. Create position IDs for input sequence.\n",
    "            2. Create token and position embeddings.\n",
    "            3. Combine token and position embeddings.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"input_size: {input_ids.size()}\")\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        logger.debug(f\"token_embd_size: {token_embeddings.size()}\")\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        logger.debug(f\"position_embd_size: {token_embeddings.size()}\")\n",
    "        \n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        logger.debug(f\"embd_size: {token_embeddings.size()}\")\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8300e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-07-27 15:42:40.460\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m34\u001B[0m - \u001B[34m\u001B[1minput_size: torch.Size([1, 9])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:42:40.460\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m38\u001B[0m - \u001B[34m\u001B[1mtoken_embd_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:42:40.460\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m40\u001B[0m - \u001B[34m\u001B[1mposition_embd_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:42:40.461\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m45\u001B[0m - \u001B[34m\u001B[1membd_size: torch.Size([1, 9, 768])\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embeddings(config)\n",
    "_ = embedding_layer(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3c4a9",
   "metadata": {},
   "source": [
    "1. In the `__init__` method of the `Embeddings` class, two embedding layers are defined: `self.token_embeddings` and `self.position_embeddings`. These layers are instances of `nn.Embedding` and are initialized with different vocabulary sizes and hidden sizes.\n",
    "\n",
    "2. In the `forward` method, the input sequence `input_ids` is passed as an argument. The size of the input sequence is determined using `input_ids.size(1)` and stored in `seq_length`.\n",
    "\n",
    "3. Position IDs are created using `torch.arange(seq_length, dtype=torch.long).unsqueeze(0)`. This creates a tensor of sequential integers from 0 to `seq_length - 1` and unsqueezes it to have a shape of `[1, seq_length]`. These position IDs represent the positions of the tokens in the input sequence.\n",
    "\n",
    "4. The token embeddings for the input sequence are obtained by passing `input_ids` to `self.token_embeddings`. This maps each token ID to its corresponding embedding vector.\n",
    "\n",
    "5. The position embeddings for the input sequence are obtained by passing `position_ids` to `self.position_embeddings`. This maps each position ID to its corresponding embedding vector.\n",
    "\n",
    "6. The token embeddings and position embeddings are added element-wise (`token_embeddings + position_embeddings`) to create the combined embeddings. This operation incorporates both the token information and the positional information of each token in the input sequence.\n",
    "\n",
    "7. The combined embeddings are then passed through `self.layer_norm`, which applies layer normalization to normalize the embeddings along the hidden dimension.\n",
    "\n",
    "8. A dropout layer, `self.dropout`, is applied to the normalized embeddings to prevent overfitting by randomly dropping out some elements.\n",
    "\n",
    "9. The resulting embeddings are returned as the output of the `forward` method.\n",
    "\n",
    "\n",
    "Note that the previous code assumes that the positional embeddings are learned embeddings, and it does not directly define the specific sinusoidal positional embeddings mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9738f",
   "metadata": {},
   "source": [
    "### Putting all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bfb5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer encoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len].\n",
    "            mask: Optional mask tensor. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim], \n",
    "            representing the output of the encoder.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc7bec49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "encoder = TransformerEncoder(config)\n",
    "encoder_output = encoder(inputs.input_ids)\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08cac0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2112, -1.0971,  0.6207,  ..., -0.0322, -0.1413, -0.0000],\n",
       "         [ 0.3913, -0.4309,  0.5121,  ..., -0.7229, -1.2283, -0.2651],\n",
       "         [ 0.0000, -0.4601,  0.1352,  ...,  0.1401, -0.3543, -0.1372],\n",
       "         ...,\n",
       "         [ 0.5117,  0.3125, -0.0207,  ...,  0.2547, -0.2552, -0.6708],\n",
       "         [ 0.5857, -0.0468, -0.2946,  ...,  0.0000,  0.2407, -0.9026],\n",
       "         [ 0.4352, -0.1858, -0.3051,  ...,  0.5849, -0.4693,  0.2345]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7e85d",
   "metadata": {},
   "source": [
    "### Adding a Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b66eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for Sequence Classification module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer for sequence classification model.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, num_labels], \n",
    "            representing the logits for each class.\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d525c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4699, -0.6738,  0.6459]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 3\n",
    "encoder_classifier = TransformerForSequenceClassification(config)\n",
    "encoder_classifier(inputs.input_ids)  # unnormalized logits for each class in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f9ac2",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b0e0d",
   "metadata": {},
   "source": [
    "The main difference between the decoder and encoder is that the decoder has two attention sublayers.\n",
    "\n",
    "The first attention sublayer, known as the _self-attention sublayer_, allows the decoder to attend to its own previously generated tokens, capturing dependencies and relationships within the output sequence. The second attention sublayer is the _encoder-decoder attention_, which allows the decoder to attend to the encoded representations produced by the encoder, incorporating contextual information from the input sequence. These attention sublayers play a crucial role in the decoder's ability to generate coherent and contextually appropriate output based on the input and the generated context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41b04337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder layer module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the decoder layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn_1 = MultiHeadAttention(config) \n",
    "        self.attn_2 = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        encoder_output: torch.Tensor,\n",
    "        source_mask: Optional[torch.Tensor] = None,\n",
    "        target_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer decoder block.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "            encoder_output: Output tensor from the encoder of shape [batch_size, seq_len, hidden_dim].\n",
    "            source_mask: Optional source mask tensor. Default is None.\n",
    "            target_mask: mask: Optional target mask tensor. Default is None.\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim], \n",
    "            representing the output of the decoder block.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"decoder_block_input_size: {x.size()}\")\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "\n",
    "        attn_1_out = self.attn_1(hidden_state, hidden_state, hidden_state, target_mask)\n",
    "        x = x + self.dropout(attn_1_out)\n",
    "        x = self.layer_norm_2(x) \n",
    "\n",
    "        attn_2_out = self.attn_2(x, encoder_output, encoder_output, source_mask)\n",
    "        x = x + self.dropout(attn_2_out)\n",
    "        x = self.layer_norm_3(x) \n",
    "\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(feed_forward_output)\n",
    "        logger.debug(f\"decoder_block_output_size: {x.size()} \")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89499fc3",
   "metadata": {},
   "source": [
    "The mask is applied in the self-attention mechanism to enforce the causality constraint during the decoding process. Since the decoder generates the target sequence autoregressively, each position in the target sequence should only attend to previous positions and not future positions. This prevents information leakage from future positions, ensuring that the model generates output in an autoregressive manner.\n",
    "\n",
    "On the other hand, the encoder-decoder attention operation in the decoder, which attends over the encoder output, doesn't require a mask because it doesn't have the same causality constraint. The cross-attention allows the decoder to attend to all positions in the encoder output, capturing relevant information from the source sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c2b0859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = inputs.input_ids.size(-1)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ffd706",
   "metadata": {},
   "source": [
    "If you recall the `scaled_dot_product_attention` function, we set the uppear values to infinity. This guarantees that the attention weights are all zero once we take the softmax over the scores (as $e^{-\\infty}= 0$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "489aaa93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-07-27 15:44:28.537\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m12\u001B[0m - \u001B[34m\u001B[1mhidden_dim: 768\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.539\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m13\u001B[0m - \u001B[34m\u001B[1mnum_heads: 12\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.540\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m17\u001B[0m - \u001B[34m\u001B[1mhead_dim: 64\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.551\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m12\u001B[0m - \u001B[34m\u001B[1mhidden_dim: 768\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.551\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m13\u001B[0m - \u001B[34m\u001B[1mnum_heads: 12\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.552\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m17\u001B[0m - \u001B[34m\u001B[1mhead_dim: 64\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.577\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m38\u001B[0m - \u001B[34m\u001B[1mdecoder_block_input_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.579\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m48\u001B[0m - \u001B[34m\u001B[1mq_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.579\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m49\u001B[0m - \u001B[34m\u001B[1mk_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.579\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m50\u001B[0m - \u001B[34m\u001B[1mv_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.580\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m56\u001B[0m - \u001B[34m\u001B[1mqT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.580\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m57\u001B[0m - \u001B[34m\u001B[1mkT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.580\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m58\u001B[0m - \u001B[34m\u001B[1mvT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.581\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m22\u001B[0m - \u001B[34m\u001B[1mquery_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.581\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m23\u001B[0m - \u001B[34m\u001B[1mkey: torch.Size([1, 12, 64, 9])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.582\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mattn_scores: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.583\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1moutput_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.584\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m48\u001B[0m - \u001B[34m\u001B[1mq_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.584\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m49\u001B[0m - \u001B[34m\u001B[1mk_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.585\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m50\u001B[0m - \u001B[34m\u001B[1mv_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.585\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m56\u001B[0m - \u001B[34m\u001B[1mqT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.585\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m57\u001B[0m - \u001B[34m\u001B[1mkT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.586\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m58\u001B[0m - \u001B[34m\u001B[1mvT_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.586\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m22\u001B[0m - \u001B[34m\u001B[1mquery_size: torch.Size([1, 12, 9, 64])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.586\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mscaled_dot_product_attention\u001B[0m:\u001B[36m23\u001B[0m - \u001B[34m\u001B[1mkey: torch.Size([1, 12, 64, 9])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.587\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mattn_scores: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.588\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1moutput_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.591\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m30\u001B[0m - \u001B[34m\u001B[1mff_output_size: torch.Size([1, 9, 768])\u001B[0m\n",
      "\u001B[32m2023-07-27 15:44:28.592\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mforward\u001B[0m:\u001B[36m51\u001B[0m - \u001B[34m\u001B[1mdecoder_block_output_size: torch.Size([1, 9, 768]) \u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4448, -0.7806, -0.0674,  ...,  0.2934,  0.0953,  0.2780],\n",
       "         [ 0.1839,  0.1398,  0.3562,  ..., -0.0375, -1.3261, -0.3123],\n",
       "         [-0.0157, -0.4480, -0.0162,  ...,  0.0030, -0.9646,  0.1143],\n",
       "         ...,\n",
       "         [ 0.7106,  0.5281,  0.2768,  ...,  0.2146, -0.2368, -0.8534],\n",
       "         [-0.0257, -0.2485, -0.4623,  ...,  0.2212, -0.1476, -0.7523],\n",
       "         [ 0.3837, -0.2680, -0.3011,  ...,  0.5106, -0.7389,  0.2627]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "decoder_layer = TransformerDecoderBlock(config)\n",
    "decoder_layer(encoder_output, encoder_output, target_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97e026b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        \"\"\"\n",
    "        Transformer Decoder module.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object for the decoder.\n",
    "            mask: Masking object for attention layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        encoder_output: torch.Tensor,\n",
    "        source_mask: torch.Tensor = None,\n",
    "        target_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, tgt_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, tgt_len, vocab_size], \n",
    "            representing the predicted probabilities over the vocabulary.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, source_mask=source_mask, target_mask=target_mask) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ec02197",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "encoder = TransformerEncoder(config)\n",
    "encoder_output = encoder(inputs.input_ids)\n",
    "decoder = TransformerDecoder(config)\n",
    "output = decoder(inputs.input_ids, encoder_output, target_mask=mask)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25568dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8122, -0.4533,  2.1675,  ...,  0.3536, -0.0078,  0.3310],\n",
       "         [ 0.0167,  0.6473, -0.9278,  ...,  1.1630,  2.0164,  0.4868],\n",
       "         [ 1.1181,  0.5270, -0.4915,  ...,  0.5112,  2.1158, -0.9579],\n",
       "         ...,\n",
       "         [ 0.3959, -0.0088, -0.1022,  ...,  0.1227,  1.2568,  0.2252],\n",
       "         [ 1.0487,  1.2220,  0.1334,  ...,  0.2272,  1.4226, -1.4288],\n",
       "         [-0.5014,  0.0110,  0.2871,  ...,  0.6070,  0.5336, -0.5335]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101efe3",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341bd75",
   "metadata": {},
   "source": [
    "Let's put it all together to create a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "440d0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder model that combines the TransformerEncoder and TransformerDecoder.\n",
    "\n",
    "    Args:\n",
    "        encoder_config: Configuration for the encoder.\n",
    "        decoder_config: Configuration for the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        config, \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        target_ids: torch.Tensor,\n",
    "        source_mask: Optional[torch.Tensor] = None,\n",
    "        target_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the encoder-decoder model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input tensor of shape [batch_size, src_len].\n",
    "            target_ids: Target tensor of shape [batch_size, tgt_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, tgt_len, vocab_size], \n",
    "            representing the predicted probabilities over the vocabulary.\n",
    "        \"\"\"\n",
    "        encoder_output = self.encoder(input_ids)\n",
    "        decoder_output = self.decoder(\n",
    "            target_ids, \n",
    "            encoder_output, \n",
    "            source_mask=source_mask, \n",
    "            target_mask=target_mask\n",
    "        )\n",
    "        x = self.fc(decoder_output)  # Apply linear layer to transform to vocab_size\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8015e",
   "metadata": {},
   "source": [
    "#### Masking\n",
    "\n",
    "The mask used in the Transformer model should have a specific shape and values to ensure proper masking during the attention mechanism. Here's how you can define the mask:\n",
    "\n",
    "1. **Padding Mask:** The padding mask is used to mask out padding tokens in the input sequences. It should have a shape of `(batch_size, seq_length)` and contain 1 where the padding tokens are present and 0 for the non-padding tokens. This mask ensures that the padding tokens do not contribute to the attention scores.\n",
    "\n",
    "2. **Future Mask:** The future mask is used to prevent attending to future positions in the self-attention mechanism. It should have a shape of `(seq_length, seq_length)` and have 1 for positions that can be attended and 0 for positions that should be masked or ignored.\n",
    "\n",
    "3. **Combined Mask:** To create the final mask, you need to combine the padding mask and the future mask. This can be done by applying logical operations, such as element-wise multiplication or logical OR, to the two masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b78ca",
   "metadata": {},
   "source": [
    "Let's verify the encoder-decoder with some fake data and future masking only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "789e98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        hidden_size: Size of the hidden state.\n",
    "        intermediate_size: Size of the intermediate layer in the feed-forward network.\n",
    "        num_hidden_layers: Number of hidden layers in the Transformer.\n",
    "        vocab_size: Size of the vocabulary.\n",
    "        max_position_embeddings: Maximum number of positional embeddings.\n",
    "        hidden_dropout_prob: Dropout probability for the hidden layers.\n",
    "        num_attention_heads: Number of attention heads in the multi-head attention.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_size: int, \n",
    "        intermediate_size: int, \n",
    "        num_hidden_layers: int, \n",
    "        vocab_size: int, \n",
    "        max_position_embeddings: int, \n",
    "        hidden_dropout_prob: float,\n",
    "        num_attention_heads: int\n",
    "    ):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob        \n",
    "        self.num_attention_heads = num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd707845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters and configuration\n",
    "config = TransformerConfig(\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2048,\n",
    "    num_hidden_layers=6,\n",
    "    vocab_size=100,\n",
    "    max_position_embeddings=512,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_attention_heads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "333b7e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10]), torch.Size([16, 12]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some fake data\n",
    "batch_size = 16\n",
    "source_length = 10\n",
    "target_length = 12\n",
    "\n",
    "source_ids = torch.randint(0, config.vocab_size, (batch_size, source_length))\n",
    "target_ids = torch.randint(0, config.vocab_size, (batch_size, target_length))\n",
    "\n",
    "source_ids.size(), target_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cba7b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(batch_size: int, seq_length: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a lower triangular mask with ones below the diagonal.\n",
    "\n",
    "    Args:\n",
    "        batch_size: The batch size.\n",
    "        seq_length: The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        The mask tensor with shape (batch_size, seq_length, seq_length).\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_length, seq_length))\n",
    "    mask = mask.unsqueeze(0).expand(batch_size, seq_length, seq_length)  # Expand the mask along the batch dimension\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05bdec11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 10]), torch.Size([16, 12, 12]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_mask = create_mask(batch_size, source_length)\n",
    "target_mask = create_mask(batch_size, target_length)\n",
    "source_mask.size(), target_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "216dbee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00f91a22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "encoder_output = encoder(source_ids)\n",
    "decoder = TransformerDecoder(config)\n",
    "output = decoder(source_ids, encoder_output, source_mask=source_mask)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd0eb382",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([16, 12, 100])\n"
     ]
    }
   ],
   "source": [
    "# Define the EncoderDecoder model\n",
    "encoder_decoder = EncoderDecoder(config)\n",
    "output = encoder_decoder(source_ids, target_ids, target_mask=target_mask)\n",
    "print(\"Output Shape:\", output.shape)  # Should be (batch_size, target_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "872851b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 12])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17704bea",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Data set generation and training from [Avanced Deep Learning with Python](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter08/transformer.py#L326\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0a9c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Provides random data copy dataset for training.\n",
    "\n",
    "    Args:\n",
    "        vocabulary_size: The vocabulary size.\n",
    "        batch_size: The batch size.\n",
    "        num_samples: The number of samples.\n",
    "        sample_length: The length of each sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_size: int, batch_size: int, num_samples: int, sample_length: int):\n",
    "        self.samples = list()\n",
    "\n",
    "        for i in range(batch_size * num_samples):\n",
    "            data = torch.from_numpy(np.random.randint(1, vocabulary_size, size=(sample_length,)))\n",
    "            data[0] = 1\n",
    "            source = torch.autograd.Variable(data, requires_grad=False)\n",
    "            target = torch.autograd.Variable(data, requires_grad=False)\n",
    "\n",
    "            # Prepare the sample dictionary\n",
    "            sample = {\n",
    "                'source': source,\n",
    "                'target': target[:-1],\n",
    "                'target_y': target[1:],\n",
    "                'source_mask': (source != 0).unsqueeze(-2),\n",
    "                'target_mask': self.make_std_mask(target, 0),\n",
    "                'tokens_count': (target[1:] != 0).data.sum()  # Assuming target_y is the actual target shifted by 1\n",
    "            }\n",
    "\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            The number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the source, target, target_y, source_mask, target_mask, and tokens_count.\n",
    "        \"\"\"\n",
    "        return self.samples[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(target: torch.Tensor, pad: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a mask to hide padding and future words.\n",
    "\n",
    "        Args:\n",
    "            target (torch.Tensor): The target tensor.\n",
    "            pad (int): The padding value.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mask tensor.\n",
    "        \"\"\"\n",
    "        target_mask = (target != pad)\n",
    "        target_mask = target_mask & torch.autograd.Variable(\n",
    "            RandomDataset.subsequent_mask(target.size(-1)).type_as(target_mask.data))\n",
    "\n",
    "        return target_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def subsequent_mask(size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask out subsequent positions.\n",
    "\n",
    "        Args:\n",
    "            size: The size of the mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The subsequent mask tensor.\n",
    "        \"\"\"\n",
    "        attn_shape = (size, size)\n",
    "        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "        return torch.from_numpy(subsequent_mask) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59397b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_samples = 1000\n",
    "samples_len = 10\n",
    "train_set = RandomDataset(config.vocab_size, batch_size, num_samples, samples_len)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82807184",
   "metadata": {},
   "source": [
    "- Inside the training loop, we iterate over the dataset batches and perform the forward pass using `model` with the appropriate input sequences and masks.\n",
    "\n",
    "- The loss is computed based on the output of the model and the target batch.\n",
    "\n",
    "- Backpropagation is performed by calling `loss.backward()`.\n",
    "\n",
    "- The optimizer's parameters are updated using `optimizer.step()` and the gradients are reset using `optimizer.zero_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85c8aeb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 5; Loss: 4.172111\n",
      "Batch: 10; Loss: 3.737579\n",
      "Batch: 15; Loss: 2.513162\n",
      "Batch: 20; Loss: 2.348919\n",
      "Batch: 25; Loss: 2.237202\n",
      "Batch: 30; Loss: 2.113222\n",
      "Batch: 35; Loss: 1.482312\n",
      "Batch: 40; Loss: 0.713707\n",
      "Batch: 45; Loss: 0.354369\n",
      "Batch: 50; Loss: 0.063452\n",
      "Batch: 55; Loss: 0.005703\n",
      "Batch: 60; Loss: 0.002458\n",
      "Batch: 65; Loss: 0.000987\n",
      "Batch: 70; Loss: 0.000610\n",
      "Batch: 75; Loss: 0.000323\n",
      "Batch: 80; Loss: 0.000215\n",
      "Batch: 85; Loss: 0.000146\n",
      "Batch: 90; Loss: 0.000089\n",
      "Batch: 95; Loss: 0.000067\n",
      "Batch: 100; Loss: 0.000059\n",
      "Batch: 105; Loss: 0.000050\n",
      "Batch: 110; Loss: 0.000044\n",
      "Batch: 115; Loss: 0.000041\n",
      "Batch: 120; Loss: 0.000038\n",
      "Batch: 125; Loss: 0.000036\n",
      "Batch: 130; Loss: 0.000035\n",
      "Batch: 135; Loss: 0.000036\n",
      "Batch: 140; Loss: 0.000032\n",
      "Batch: 145; Loss: 0.000030\n",
      "Batch: 150; Loss: 0.000030\n",
      "Batch: 155; Loss: 0.000029\n",
      "Batch: 160; Loss: 0.000027\n",
      "Batch: 165; Loss: 0.000029\n",
      "Batch: 170; Loss: 0.000025\n",
      "Batch: 175; Loss: 0.000024\n",
      "Batch: 180; Loss: 0.000024\n",
      "Batch: 185; Loss: 0.000022\n",
      "Batch: 190; Loss: 0.000037\n",
      "Batch: 195; Loss: 0.000023\n",
      "Batch: 200; Loss: 0.000023\n",
      "Batch: 205; Loss: 0.000022\n",
      "Batch: 210; Loss: 0.000022\n",
      "Batch: 215; Loss: 0.000020\n",
      "Batch: 220; Loss: 0.000021\n",
      "Batch: 225; Loss: 0.000019\n",
      "Batch: 230; Loss: 0.000020\n",
      "Batch: 235; Loss: 0.000017\n",
      "Batch: 240; Loss: 0.000017\n",
      "Batch: 245; Loss: 0.000018\n",
      "Batch: 250; Loss: 0.000017\n",
      "Batch: 255; Loss: 0.000016\n",
      "Batch: 260; Loss: 0.000015\n",
      "Batch: 265; Loss: 0.000016\n",
      "Batch: 270; Loss: 0.000014\n",
      "Batch: 275; Loss: 0.000014\n",
      "Batch: 280; Loss: 0.000014\n",
      "Batch: 285; Loss: 0.000013\n",
      "Batch: 290; Loss: 0.000013\n",
      "Batch: 295; Loss: 0.000013\n",
      "Batch: 300; Loss: 0.000012\n",
      "Batch: 305; Loss: 0.000012\n",
      "Batch: 310; Loss: 0.000012\n",
      "Batch: 315; Loss: 0.000012\n",
      "Batch: 320; Loss: 0.000013\n",
      "Batch: 325; Loss: 0.000011\n",
      "Batch: 330; Loss: 0.000011\n",
      "Batch: 335; Loss: 0.000011\n",
      "Batch: 340; Loss: 0.000011\n",
      "Batch: 345; Loss: 0.000010\n",
      "Batch: 350; Loss: 0.000010\n",
      "Batch: 355; Loss: 0.000010\n",
      "Batch: 360; Loss: 0.000010\n",
      "Batch: 365; Loss: 0.000010\n",
      "Batch: 370; Loss: 0.000010\n",
      "Batch: 375; Loss: 0.000009\n",
      "Batch: 380; Loss: 0.000009\n",
      "Batch: 385; Loss: 0.000009\n",
      "Batch: 390; Loss: 0.000009\n",
      "Batch: 395; Loss: 0.000009\n",
      "Batch: 400; Loss: 0.000009\n",
      "Batch: 405; Loss: 0.000008\n",
      "Batch: 410; Loss: 0.000008\n",
      "Batch: 415; Loss: 0.000008\n",
      "Batch: 420; Loss: 0.000008\n",
      "Batch: 425; Loss: 0.000009\n",
      "Batch: 430; Loss: 0.000008\n",
      "Batch: 435; Loss: 0.000008\n",
      "Batch: 440; Loss: 0.000008\n",
      "Batch: 445; Loss: 0.000007\n",
      "Batch: 450; Loss: 0.000007\n",
      "Batch: 455; Loss: 0.000007\n",
      "Batch: 460; Loss: 0.000007\n",
      "Batch: 465; Loss: 0.000007\n",
      "Batch: 470; Loss: 0.000007\n",
      "Batch: 475; Loss: 0.000007\n",
      "Batch: 480; Loss: 0.000007\n",
      "Batch: 485; Loss: 0.000007\n",
      "Batch: 490; Loss: 0.000007\n",
      "Batch: 495; Loss: 0.000007\n",
      "Batch: 500; Loss: 0.000006\n",
      "Batch: 505; Loss: 0.000007\n",
      "Batch: 510; Loss: 0.000006\n",
      "Batch: 515; Loss: 0.000006\n",
      "Batch: 520; Loss: 0.000006\n",
      "Batch: 525; Loss: 0.000006\n",
      "Batch: 530; Loss: 0.000006\n",
      "Batch: 535; Loss: 0.000006\n",
      "Batch: 540; Loss: 0.000006\n",
      "Batch: 545; Loss: 0.000008\n",
      "Batch: 550; Loss: 0.000006\n",
      "Batch: 555; Loss: 0.000005\n",
      "Batch: 560; Loss: 0.000005\n",
      "Batch: 565; Loss: 0.000005\n",
      "Batch: 570; Loss: 0.000005\n",
      "Batch: 575; Loss: 0.000005\n",
      "Batch: 580; Loss: 0.000005\n",
      "Batch: 585; Loss: 0.000005\n",
      "Batch: 590; Loss: 0.000005\n",
      "Batch: 595; Loss: 0.000005\n",
      "Batch: 600; Loss: 0.000005\n",
      "Batch: 605; Loss: 0.000005\n",
      "Batch: 610; Loss: 0.000005\n",
      "Batch: 615; Loss: 0.000005\n",
      "Batch: 620; Loss: 0.000005\n",
      "Batch: 625; Loss: 0.000004\n",
      "Batch: 630; Loss: 0.000005\n",
      "Batch: 635; Loss: 0.000004\n",
      "Batch: 640; Loss: 0.000004\n",
      "Batch: 645; Loss: 0.000004\n",
      "Batch: 650; Loss: 0.000004\n",
      "Batch: 655; Loss: 0.000005\n",
      "Batch: 660; Loss: 0.000004\n",
      "Batch: 665; Loss: 0.000004\n",
      "Batch: 670; Loss: 0.000004\n",
      "Batch: 675; Loss: 0.000004\n",
      "Batch: 680; Loss: 0.000004\n",
      "Batch: 685; Loss: 0.000004\n",
      "Batch: 690; Loss: 0.000004\n",
      "Batch: 695; Loss: 0.000004\n",
      "Batch: 700; Loss: 0.000004\n",
      "Batch: 705; Loss: 0.000005\n",
      "Batch: 710; Loss: 0.000004\n",
      "Batch: 715; Loss: 0.000004\n",
      "Batch: 720; Loss: 0.000004\n",
      "Batch: 725; Loss: 0.000004\n",
      "Batch: 730; Loss: 0.000004\n",
      "Batch: 735; Loss: 0.000004\n",
      "Batch: 740; Loss: 0.000004\n",
      "Batch: 745; Loss: 0.000003\n",
      "Batch: 750; Loss: 0.000003\n",
      "Batch: 755; Loss: 0.000004\n",
      "Batch: 760; Loss: 0.000003\n",
      "Batch: 765; Loss: 0.000004\n",
      "Batch: 770; Loss: 0.000003\n",
      "Batch: 775; Loss: 0.000004\n",
      "Batch: 780; Loss: 0.000003\n",
      "Batch: 785; Loss: 0.000003\n",
      "Batch: 790; Loss: 0.000003\n",
      "Batch: 795; Loss: 0.000003\n",
      "Batch: 800; Loss: 0.000003\n",
      "Batch: 805; Loss: 0.000003\n",
      "Batch: 810; Loss: 0.000003\n",
      "Batch: 815; Loss: 0.000003\n",
      "Batch: 820; Loss: 0.000003\n",
      "Batch: 825; Loss: 0.000003\n",
      "Batch: 830; Loss: 0.000003\n",
      "Batch: 835; Loss: 0.000003\n",
      "Batch: 840; Loss: 0.000003\n",
      "Batch: 845; Loss: 0.000003\n",
      "Batch: 850; Loss: 0.000003\n",
      "Batch: 855; Loss: 0.000003\n",
      "Batch: 860; Loss: 0.000003\n",
      "Batch: 865; Loss: 0.000003\n",
      "Batch: 870; Loss: 0.000003\n",
      "Batch: 875; Loss: 0.000003\n",
      "Batch: 880; Loss: 0.000003\n",
      "Batch: 885; Loss: 0.000003\n",
      "Batch: 890; Loss: 0.000003\n",
      "Batch: 895; Loss: 0.000003\n",
      "Batch: 900; Loss: 0.000003\n",
      "Batch: 905; Loss: 0.000003\n",
      "Batch: 910; Loss: 0.000003\n",
      "Batch: 915; Loss: 0.000003\n",
      "Batch: 920; Loss: 0.000003\n",
      "Batch: 925; Loss: 0.000002\n",
      "Batch: 930; Loss: 0.000002\n",
      "Batch: 935; Loss: 0.000003\n",
      "Batch: 940; Loss: 0.000002\n",
      "Batch: 945; Loss: 0.000002\n",
      "Batch: 950; Loss: 0.000002\n",
      "Batch: 955; Loss: 0.000002\n",
      "Batch: 960; Loss: 0.000002\n",
      "Batch: 965; Loss: 0.000002\n",
      "Batch: 970; Loss: 0.000002\n",
      "Batch: 975; Loss: 0.000002\n",
      "Batch: 980; Loss: 0.000002\n",
      "Batch: 985; Loss: 0.000002\n",
      "Batch: 990; Loss: 0.000002\n",
      "Batch: 995; Loss: 0.000002\n",
      "Batch: 1000; Loss: 0.000002\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoder(config)\n",
    "\n",
    "# Initialize parameters.\n",
    "for p in encoder_decoder.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(p)\n",
    "        \n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "current_loss = 0.0\n",
    "counter = 0\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    with torch.set_grad_enabled(True):\n",
    "        out = model.forward(batch['source'], batch['target'], batch['source_mask'], batch['target_mask'])\n",
    "        loss = loss_function(out.contiguous().view(-1, out.size(-1)), batch['target_y'].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        current_loss += loss\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 5 == 0:\n",
    "            print(\"Batch: %d; Loss: %f\" % (i + 1, current_loss / counter))\n",
    "            current_loss = 0.0\n",
    "            counter = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
